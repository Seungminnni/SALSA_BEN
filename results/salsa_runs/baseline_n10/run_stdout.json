{
  "returncode": 1,
  "stdout": [
    "\ud558\uc704 \ub514\ub809\ud130\ub9ac \ub610\ub294 \ud30c\uc77c -p\uc774(\uac00) \uc774\ubbf8 \uc788\uc2b5\ub2c8\ub2e4.\n",
    "\ub2e4\uc74c \ub0b4\uc6a9 \uc9c4\ud589 \uc911 \uc624\ub958 \ubc1c\uc0dd: -p.\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - ============ Initialized logger ============\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - A_shift: 0\n",
    "                                     N: 10\n",
    "                                     Q: 842779\n",
    "                                     angular_emb: False\n",
    "                                     attention_dropout: 0\n",
    "                                     base: 81\n",
    "                                     bucket_size: 1\n",
    "                                     check_secret_every: 2000\n",
    "                                     ckpt_path: C:\\checkpoint\\user\\dumped\\baseline_n10_exp\\58cvxxeab3\\ckpt.json\n",
    "                                     clip_grad_norm: 5.0\n",
    "                                     command: python C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\external\\LWE-benchmarking\\src\\salsa\\train_and_recover.py --data_path 'C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\data\\precomputed\\baseline_n10' --exp_name baseline_n10_exp --secret_seed 0 --rlwe 0 --task lwe --hamming 3 --cpu false --dtype float16 --train_batch_size 128 --val_batch_size 256 --n_enc_heads 8 --n_enc_layers 4 --enc_emb_dim 256 --epochs 5 --exp_id \"58cvxxeab3\"\n",
    "                                     compile: True\n",
    "                                     cpu: False\n",
    "                                     data_path: C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\data\\precomputed\\baseline_n10\n",
    "                                     debug_slurm: False\n",
    "                                     device: cuda:0\n",
    "                                     distinguisher_size: 128\n",
    "                                     dropout: 0\n",
    "                                     dtype: float16\n",
    "                                     dump_path: C:\\checkpoint\\user\\dumped\\baseline_n10_exp\\58cvxxeab3\n",
    "                                     dxdistinguisher: False\n",
    "                                     enc_emb_dim: 256\n",
    "                                     epochs: 5\n",
    "                                     exp_id: 58cvxxeab3\n",
    "                                     exp_name: baseline_n10_exp\n",
    "                                     gamma: 2\n",
    "                                     global_rank: 0\n",
    "                                     hamming: 3\n",
    "                                     is_master: True\n",
    "                                     is_slurm_job: False\n",
    "                                     local_rank: 0\n",
    "                                     log_every: 100\n",
    "                                     master_port: 10035\n",
    "                                     matrix_emb: False\n",
    "                                     max_hours: 70\n",
    "                                     max_samples: None\n",
    "                                     multi_gpu: False\n",
    "                                     multi_node: False\n",
    "                                     n_enc_heads: 8\n",
    "                                     n_enc_layers: 4\n",
    "                                     n_gpu_per_node: 1\n",
    "                                     n_nodes: 1\n",
    "                                     node_id: 0\n",
    "                                     optimizer: adam_warmup,lr=0.00001,warmup_updates=8000,weight_decay=0.001\n",
    "                                     patch_size: None\n",
    "                                     recover_only: False\n",
    "                                     rlwe: 0\n",
    "                                     run_id: \n",
    "                                     save_every: 10000\n",
    "                                     save_periodic: 0\n",
    "                                     secret_seed: 0\n",
    "                                     secret_type: ternary\n",
    "                                     seed: 1759087242\n",
    "                                     shuffle: True\n",
    "                                     sigma: 3.0\n",
    "                                     slurm_id: None\n",
    "                                     stacked_circulants: True\n",
    "                                     task: lwe\n",
    "                                     timescale: 40\n",
    "                                     train_batch_size: 128\n",
    "                                     val_batch_size: 256\n",
    "                                     val_every: 60000\n",
    "                                     workers: 8\n",
    "                                     world_size: 1\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - The experiment will be stored in C:\\checkpoint\\user\\dumped\\baseline_n10_exp\\58cvxxeab3\n",
    "                                     \n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - Running command: python C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\external\\LWE-benchmarking\\src\\salsa\\train_and_recover.py --data_path 'C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\data\\precomputed\\baseline_n10' --exp_name baseline_n10_exp --secret_seed 0 --rlwe 0 --task lwe --hamming 3 --cpu false --dtype float16 --train_batch_size 128 --val_batch_size 256 --n_enc_heads 8 --n_enc_layers 4 --enc_emb_dim 256 --epochs 5\n",
    "\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - vocabulary: 81 words\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - words: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14, '15': 15, '16': 16, '17': 17, '18': 18, '19': 19, '20': 20, '21': 21, '22': 22, '23': 23, '24': 24, '25': 25, '26': 26, '27': 27, '28': 28, '29': 29, '30': 30, '31': 31, '32': 32, '33': 33, '34': 34, '35': 35, '36': 36, '37': 37, '38': 38, '39': 39, '40': 40, '41': 41, '42': 42, '43': 43, '44': 44, '45': 45, '46': 46, '47': 47, '48': 48, '49': 49, '50': 50, '51': 51, '52': 52, '53': 53, '54': 54, '55': 55, '56': 56, '57': 57, '58': 58, '59': 59, '60': 60, '61': 61, '62': 62, '63': 63, '64': 64, '65': 65, '66': 66, '67': 67, '68': 68, '69': 69, '70': 70, '71': 71, '72': 72, '73': 73, '74': 74, '75': 75, '76': 76, '77': 77, '78': 78, '79': 79, '80': 80}\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - Loaded A and b. [split: test]\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - Loaded data. [root: C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\data\\precomputed\\baseline_n10, count: 128]\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - Loaded A and b. [split: orig]\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - Loaded data. [root: C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\data\\precomputed\\baseline_n10, count: 500]\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - Loaded A and b. [split: train]\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - Loaded data. [root: C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\data\\precomputed\\baseline_n10, count: 500]\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - Optimizer: AdamWithWarmup\n",
    "C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\external\\LWE-benchmarking\\src\\salsa\\train\\trainer.py:79: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
    "  self.scaler = torch.cuda.amp.GradScaler(enabled=enabled)\n",
    "INFO - 09/29/25 04:20:42 - 0:00:00 - No checkpoint found at C:\\checkpoint\\user\\dumped\\baseline_n10_exp\\58cvxxeab3\\checkpoint.pth.\n",
    "INFO - 09/29/25 04:20:43 - 0:00:01 - ============ Starting epoch 0 ... ============\n",
    "W0929 04:21:00.551000 31260 site-packages\\torch\\_inductor\\utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
    "SLURM job: False\n",
    "0 - Number of nodes: 1\n",
    "0 - Node ID        : 0\n",
    "0 - Local rank     : 0\n",
    "0 - Global rank    : 0\n",
    "0 - World size     : 1\n",
    "0 - GPUs per node  : 1\n",
    "0 - Master         : True\n",
    "0 - Multi-node     : False\n",
    "0 - Multi-GPU      : False\n",
    "0 - Hostname       : DESKTOP-TK0K987\n",
    "Traceback (most recent call last):\n",
    "  File \"C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\external\\LWE-benchmarking\\src\\salsa\\train_and_recover.py\", line 213, in <module>\n",
    "    main(params)\n",
    "    ~~~~^^^^^^^^\n",
    "  File \"C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\external\\LWE-benchmarking\\src\\salsa\\train_and_recover.py\", line 192, in main\n",
    "    trainer.train()\n",
    "    ~~~~~~~~~~~~~^^\n",
    "  File \"C:\\Users\\user\\OneDrive\\Desktop\\salsa-repro-salsa-connected\\external\\LWE-benchmarking\\src\\salsa\\train\\trainer.py\", line 206, in train\n",
    "    logits = self.model(A)\n",
    "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
    "    return self._call_impl(*args, **kwargs)\n",
    "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
    "    return forward_call(*args, **kwargs)\n",
    "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 663, in _fn\n",
    "    raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
    "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3957, in create_backend\n",
    "    raise TritonMissing(inspect.currentframe())\n",
    "torch._inductor.exc.TritonMissing: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n",
    "\n",
    "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
    "\n"
  ]
}